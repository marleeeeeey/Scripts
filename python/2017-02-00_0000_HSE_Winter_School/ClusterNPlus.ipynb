{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pymorphy2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download a page from nplus1.ru by adress adr.\n",
    "def getArticleTextNPlus1(adr):\n",
    " r = requests.get(adr)\n",
    " time.sleep(1) # We are ethical hackers.\n",
    "\n",
    " # Extract information from the page. Using regular expressions for it easier than consider the Dom structure of the page.\n",
    " tables=re.split(\"</div>\",re.split('=\"tables\"', r.text)[1])[0]\n",
    " t1=re.split(\"</time>\", re.split(\"<time\", tables)[1])[0]\n",
    " n_time=re.split(\"</span>\", re.split(\"<span>\", t1)[1])[0]\n",
    " n_date=re.split(\"</span>\", re.split(\"<span>\", t1)[2])[0]\n",
    " n_rubr=re.split(\">\", re.split(\"</a>\", re.split(\"<a href\", tables)[1])[0])[1]\n",
    " n_diff=re.split(\"</span>\", re.split('\"difficult-value\">', tables)[1])[0]\n",
    " n_head=re.split(\"</h1>\",re.split('<h1>', r.text)[1])[0]\n",
    " n_author=re.split('\" />',re.split('<meta name=\"author\" content=\"', r.text)[1])[0]\n",
    " n_text=re.split(\"</div>\", re.split(\"</figure>\", re.split('</article>',re.split('<article', r.text)[1])[0])[1])[1]    \n",
    " n_text=re.sub(\"<br>|<br\\/>|<p>|<\\/p>\", \" \", str.lower(n_text))\n",
    "\n",
    " # We need BeautifulSoup to cut of all the tags from the extracted text.\n",
    " beaux_text=BeautifulSoup(n_text, \"lxml\")\n",
    " n_text=beaux_text.get_text() \n",
    "\n",
    " return [n_time, n_date, n_rubr, n_diff, n_author, n_head, n_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download all news for a day from nplus1.ru \n",
    "def getNPlus1Day(adr, day):\n",
    " # Requesting page using the Requests library.\n",
    " r = requests.get(adr)\n",
    " # All addresses are bordered by this tag.\n",
    " refs=re.split('<article class=\"item item-news item-news', r.text)\n",
    " database=[]\n",
    "\n",
    " # Downloading news if any.\n",
    " if len(refs)>0: \n",
    "  for t in tqdm(refs[1:], desc='day '+day):\n",
    "   href=re.split('\"', t)[6]\n",
    "   database.append(getArticleTextNPlus1(\"https://nplus1.ru\"+href))\n",
    " return database\n",
    "#def getNPlus1Day(adr, day):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in words of this parts of speech only.\n",
    "POSes=set(['NOUN', 'VERB', 'ADJF', 'PRTF', 'GRND', 'ADJS', 'PRED', 'PRCL', 'INFN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the vector of words frequencies.\n",
    "def getFreqVector(text, morph):\n",
    " # Let a Russian word be a sequence of Russian characters.\n",
    " words=re.findall(\"[А-Яа-я]+\\-[А-Яа-я]+|[А-Яа-я]+\", text)\n",
    "\n",
    " pwords=[]\n",
    " for w in words[:-1]:\n",
    "  # Gathering initial forms of the given word usin Pymorphy2.\n",
    "  prsd=morph.parse(w) \n",
    "  # Берем только значимые части речи. Так как вариантов анализа очень много, просто берем самый вероятный.\n",
    "  if prsd[0].tag.POS in POSes:\n",
    "   pwords.append(prsd[0].normal_form)\n",
    "\n",
    " # Строим словарь из начальных форм текста.\n",
    " uwords=set(pwords)\n",
    " dwords={w:0 for w in uwords}\n",
    "\n",
    " # Считаем частоты встречаемости начальных форм.\n",
    " for w in words[:-1]:\n",
    "  prsd=morph.parse(w) \n",
    "  if prsd[0].normal_form in pwords:\n",
    "   dwords[prsd[0].normal_form]+=1\n",
    "\n",
    " # Возвращаем все слова, которые встретились чаще, чем 1 раз. \n",
    " dwords2={w:dwords[w] for w in dwords.keys() if dwords[w]>1}\n",
    " return dwords2\n",
    "#def getFreqVector(text, morph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating cosine similarity (distance) between vectors. \n",
    "# 1 - text are the same, 0 - texts are sharing no any word.\n",
    "# Form formula consult https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "def cosineSimilarity(words1, words2):\n",
    " cntr=0\n",
    " for w in words1:\n",
    "  if w in words2:\n",
    "    cntr+=words1[w]*words2[w]\n",
    " cntr1=0\n",
    " for w in words1:\n",
    "  cntr1+=words1[w]*words1[w]\n",
    " cntr2=0\n",
    " for w in words2:\n",
    "  cntr2+=words2[w]*words2[w]\n",
    " if cntr1*cntr2==0:\n",
    "  return 0;\n",
    " return cntr/(math.sqrt(cntr1*cntr2))\n",
    "#def cosineSimilarity(words1, words2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "day 1: 100%|███████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "day 2: 100%|███████| 5/5 [00:06<00:00,  1.20s/it]\n",
      "day 3: 0it [00:00, ?it/s]\n",
      "day 4: 100%|███████| 8/8 [00:09<00:00,  1.16s/it]\n",
      "morph-ing: 100%|█| 14/14 [00:01<00:00,  8.60it/s]\n",
      "making everything similar: 100%|█| 14/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ 0 +++\n",
      "Программу научили искать крохотные объекты на электронных томограммах\n",
      "+++ 1 +++\n",
      "CRISPR позволила «исправить» смертельно опасные мутации во взрослом организме\n",
      "+++ 2 +++\n",
      "Apple Watch превратили в пульт управления дроном\n",
      "+++ 3 +++\n",
      "В мозге нашелся «центр неудовольствия»\n",
      "+++ 4 +++\n",
      "Под детской площадкой в Шотландии нашли скелет пирата\n",
      "+++ 5 +++\n",
      "Мелкие хамелеоны оказались мощнее крупных\n",
      "+++ 6 +++\n",
      "Windows 95 запустили на игровой консоли Nintendo 3DS \n",
      "+++ 7 +++\n",
      "Адгезию обвинили в нарушении школьных законов физики\n",
      "+++ 8 +++\n",
      "3D-печать приспособили для создания прочной термостойкой керамики\n",
      "+++ 9 +++\n",
      "Приземлившаяся первая ступень Falcon 9 готова к повторному использованию\n",
      "+++ 10 +++\n",
      "Российские физики усомнились в приоритете японцев на 113-й элемент\n",
      "+++ 11 +++\n",
      "Нанодоспехи позволили бактериям поглощать углекислый газ\n",
      "+++ 12 +++\n",
      "Транспортные РНК оказались участниками эпигенетического наследования\n",
      "[-1 -1 -1  0 -1 -1 -1 -1 -1 -1 -1  0 -1 -1]\n",
      "---None---\n",
      "Нанодоспехи позволили бактериям поглощать углекислый газ\n",
      "Российские физики усомнились в приоритете японцев на 113-й элемент\n",
      "Транспортные РНК оказались участниками эпигенетического наследования\n",
      "В мозге нашелся «центр неудовольствия»\n",
      "CRISPR позволила «исправить» смертельно опасные мутации во взрослом организме\n",
      "Адгезию обвинили в нарушении школьных законов физики\n",
      "3D-печать приспособили для создания прочной термостойкой керамики\n",
      "Мелкие хамелеоны оказались мощнее крупных\n",
      "Windows 95 запустили на игровой консоли Nintendo 3DS \n",
      "Программу научили искать крохотные объекты на электронных томограммах\n",
      "Приземлившаяся первая ступень Falcon 9 готова к повторному использованию\n",
      "Apple Watch превратили в пульт управления дроном\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "# Downloading news for 30 days of the November 2016.\n",
    "all_days=[]\n",
    "for i in range(1, 5): \n",
    " a_day = getNPlus1Day('https://nplus1.ru/news/2016/01/'+str(i)+'/', str(i))\n",
    " all_days+=a_day\n",
    "\n",
    "# Create an PyMorphy2 object for grammatical analysis.\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Creating frequency vectors for news.\n",
    "words=[]\n",
    "for t in tqdm(all_days, desc='morph-ing'):\n",
    " words.append(getFreqVector(t[6], morph))\n",
    "\n",
    "# Calculating matrix of distances for new. Comparing any news with any.\n",
    "similarity=[[0 for r in words] for r2 in words]\n",
    "for i in tqdm(range(len(words)), desc='making everything similar'):\n",
    " for j in range(i+1, len(words)):\n",
    "   sim=cosineSimilarity(words[i], words[j])\n",
    "   similarity[i][j]=1-sim # 1-sim because we need distance but not similarity. If cosine measure is equal to 1 then distance is equal to 0.\n",
    "   similarity[j][i]=1-sim\n",
    "\n",
    "# Clustering using SciPy.sklearn using k-means algorithm. \n",
    "# https://ru.wikipedia.org/wiki/K-means\n",
    "# http://scikit-learn.org/stable/modules/clustering.html\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "clusters=KMeans(n_clusters=14, precompute_distances = False).fit_predict(similarity)\n",
    "\n",
    "# Just output.\n",
    "for i in range(max(clusters)):\n",
    " print(\"+++ \"+str(i)+\" +++\")\n",
    " for l in range(len(words)):\n",
    "  if clusters[l]==i:\n",
    "   print(all_days[l][5])\n",
    "\n",
    "# Clustering using DBSCAN.\n",
    "# https://en.wikipedia.org/wiki/DBSCAN\n",
    "clusters = DBSCAN(eps=0.7, min_samples=2, metric='precomputed').fit(similarity)\n",
    "\n",
    "print(clusters.labels_)\n",
    "\n",
    "for i in range(max(clusters.labels_)):\n",
    " print(\"+++\"+str(i)+\"+++\")\n",
    " for l in range(len(words)):\n",
    "  if clusters.labels_[l]==i:\n",
    "   print(all_days[l][5])\n",
    "\n",
    "print(\"---None---\")\n",
    "for l in range(len(words)):\n",
    " if clusters.labels_[l]==-1:\n",
    "  print(all_days[l][5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
